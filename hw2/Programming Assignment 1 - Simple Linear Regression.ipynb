{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 1:  Simple Linear Regression (215 points)\n",
    "\n",
    "In this assignment, you will load data, plot data, perform simple mathematical manipulations, and fit a simple linear regression model. The assignment uses the Boston housing data set, a widely-used machine learning data set for illustrating basic concepts.  \n",
    "\n",
    "Please add your own print statements to check your code to ensure your code is correct in every step. (Note: we will not be grading the print statements you add to your code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "The Boston housing data set was collected in the 1970s to study the relationship between house price and various factors such as the house size, crime rate, socio-economic status, etc.  Since the variables are easy to understand, the data set is ideal for learning basic concepts in machine learning.  The raw data and a complete description of the dataset can be found on the UCI website:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Housing\n",
    "\n",
    "First, complete the following code that uses the `pd.read_csv` command to read the data from the file located at\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\n",
    "\n",
    "I have supplied a list `names` of the column headers.  You will have to set the options in the `read_csv` command to correctly delimit the data in the file and name the columns correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell using Shift + Enter before moving further\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =[\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', \n",
    "    'AGE',  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE'\n",
    "]\n",
    "\n",
    "# TODO:  Complete the code - 5 points\n",
    "# df = pd.read_csv(...)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
    "df = pd.read_csv(url, names = names, sep = '\\s+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first six rows of the data frame after adding the appropriate column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "5  0.02985   0.0   2.18     0  0.458  6.430  58.7  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  \n",
       "5     18.7  394.12   5.21   28.7  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Manipulations on the Data\n",
    "\n",
    "What is the shape of the data?  How many attributes are there?  How many samples?\n",
    "Print a statement of the form:\n",
    "\n",
    "    num samples=xxx, num attributes=yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If done correctly: num_samples = 506, num_attributes = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sample is: 506 number of attributes: 14\n"
     ]
    }
   ],
   "source": [
    "# TODO - 5 points\n",
    "shape = df.shape\n",
    "num_samples = shape[0]\n",
    "num_attributes = shape[1]\n",
    "print(\"number of sample is:\", num_samples, \"number of attributes:\", num_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a response vector `y` with the values in the column `PRICE`.  The vector `y` should be a 1D `numpy.array` structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n",
      "type is: <class 'numpy.ndarray'>\n",
      "shape is: (506,)\n"
     ]
    }
   ],
   "source": [
    "# TODO - 5 points \n",
    "prices = df[\"PRICE\"]\n",
    "# print(prices)\n",
    "y = np.array(prices)\n",
    "y.reshape(1,506)\n",
    "print(y)\n",
    "print(\"type is:\", type(y))\n",
    "print(\"shape is:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the response vector `y` to find the mean house price in thousands and the fraction of homes that are above $40k. (You may realize this is very cheap.  Prices have gone up a lot since the 1970s!).   Create print statements of the form:\n",
    "\n",
    "    The mean house price is xx.yy thousands of dollars.\n",
    "    Only x.y percent are above $40k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If done correctly: The mean house price is 22.53 thousands of dollars. Only 6.1 percent are above $40k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.532806324110677\n",
      "6.126482213438735\n"
     ]
    }
   ],
   "source": [
    "# TODO - 10 points\n",
    "mean = y.mean()\n",
    "print(mean)\n",
    "larger_than_40 = y[y>40] #get elements that are larger than 40\n",
    "num = larger_than_40.shape[0]\n",
    "percent = num/506\n",
    "print(percent * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "Python's `matplotlib` has very good routines for plotting and visualizing data that closely follows the format of MATLAB programs.  You can load the `matplotlib` package with the following commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `y` vector, create a predictor vector `x` containing the values in the `RM` column, which represents the average number of rooms in each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.575 6.421 7.185 6.998 7.147 6.43  6.012 6.172 5.631 6.004 6.377 6.009\n",
      " 5.889 5.949 6.096 5.834 5.935 5.99  5.456 5.727 5.57  5.965 6.142 5.813\n",
      " 5.924 5.599 5.813 6.047 6.495 6.674 5.713 6.072 5.95  5.701 6.096 5.933\n",
      " 5.841 5.85  5.966 6.595 7.024 6.77  6.169 6.211 6.069 5.682 5.786 6.03\n",
      " 5.399 5.602 5.963 6.115 6.511 5.998 5.888 7.249 6.383 6.816 6.145 5.927\n",
      " 5.741 5.966 6.456 6.762 7.104 6.29  5.787 5.878 5.594 5.885 6.417 5.961\n",
      " 6.065 6.245 6.273 6.286 6.279 6.14  6.232 5.874 6.727 6.619 6.302 6.167\n",
      " 6.389 6.63  6.015 6.121 7.007 7.079 6.417 6.405 6.442 6.211 6.249 6.625\n",
      " 6.163 8.069 7.82  7.416 6.727 6.781 6.405 6.137 6.167 5.851 5.836 6.127\n",
      " 6.474 6.229 6.195 6.715 5.913 6.092 6.254 5.928 6.176 6.021 5.872 5.731\n",
      " 5.87  6.004 5.961 5.856 5.879 5.986 5.613 5.693 6.431 5.637 6.458 6.326\n",
      " 6.372 5.822 5.757 6.335 5.942 6.454 5.857 6.151 6.174 5.019 5.403 5.468\n",
      " 4.903 6.13  5.628 4.926 5.186 5.597 6.122 5.404 5.012 5.709 6.129 6.152\n",
      " 5.272 6.943 6.066 6.51  6.25  7.489 7.802 8.375 5.854 6.101 7.929 5.877\n",
      " 6.319 6.402 5.875 5.88  5.572 6.416 5.859 6.546 6.02  6.315 6.86  6.98\n",
      " 7.765 6.144 7.155 6.563 5.604 6.153 7.831 6.782 6.556 7.185 6.951 6.739\n",
      " 7.178 6.8   6.604 7.875 7.287 7.107 7.274 6.975 7.135 6.162 7.61  7.853\n",
      " 8.034 5.891 6.326 5.783 6.064 5.344 5.96  5.404 5.807 6.375 5.412 6.182\n",
      " 5.888 6.642 5.951 6.373 6.951 6.164 6.879 6.618 8.266 8.725 8.04  7.163\n",
      " 7.686 6.552 5.981 7.412 8.337 8.247 6.726 6.086 6.631 7.358 6.481 6.606\n",
      " 6.897 6.095 6.358 6.393 5.593 5.605 6.108 6.226 6.433 6.718 6.487 6.438\n",
      " 6.957 8.259 6.108 5.876 7.454 8.704 7.333 6.842 7.203 7.52  8.398 7.327\n",
      " 7.206 5.56  7.014 8.297 7.47  5.92  5.856 6.24  6.538 7.691 6.758 6.854\n",
      " 7.267 6.826 6.482 6.812 7.82  6.968 7.645 7.923 7.088 6.453 6.23  6.209\n",
      " 6.315 6.565 6.861 7.148 6.63  6.127 6.009 6.678 6.549 5.79  6.345 7.041\n",
      " 6.871 6.59  6.495 6.982 7.236 6.616 7.42  6.849 6.635 5.972 4.973 6.122\n",
      " 6.023 6.266 6.567 5.705 5.914 5.782 6.382 6.113 6.426 6.376 6.041 5.708\n",
      " 6.415 6.431 6.312 6.083 5.868 6.333 6.144 5.706 6.031 6.316 6.31  6.037\n",
      " 5.869 5.895 6.059 5.985 5.968 7.241 6.54  6.696 6.874 6.014 5.898 6.516\n",
      " 6.635 6.939 6.49  6.579 5.884 6.728 5.663 5.936 6.212 6.395 6.127 6.112\n",
      " 6.398 6.251 5.362 5.803 8.78  3.561 4.963 3.863 4.97  6.683 7.016 6.216\n",
      " 5.875 4.906 4.138 7.313 6.649 6.794 6.38  6.223 6.968 6.545 5.536 5.52\n",
      " 4.368 5.277 4.652 5.    4.88  5.39  5.713 6.051 5.036 6.193 5.887 6.471\n",
      " 6.405 5.747 5.453 5.852 5.987 6.343 6.404 5.349 5.531 5.683 4.138 5.608\n",
      " 5.617 6.852 5.757 6.657 4.628 5.155 4.519 6.434 6.782 5.304 5.957 6.824\n",
      " 6.411 6.006 5.648 6.103 5.565 5.896 5.837 6.202 6.193 6.38  6.348 6.833\n",
      " 6.425 6.436 6.208 6.629 6.461 6.152 5.935 5.627 5.818 6.406 6.219 6.485\n",
      " 5.854 6.459 6.341 6.251 6.185 6.417 6.749 6.655 6.297 7.393 6.728 6.525\n",
      " 5.976 5.936 6.301 6.081 6.701 6.376 6.317 6.513 6.209 5.759 5.952 6.003\n",
      " 5.926 5.713 6.167 6.229 6.437 6.98  5.427 6.162 6.484 5.304 6.185 6.229\n",
      " 6.242 6.75  7.061 5.762 5.871 6.312 6.114 5.905 5.454 5.414 5.093 5.983\n",
      " 5.983 5.707 5.926 5.67  5.39  5.794 6.019 5.569 6.027 6.593 6.12  6.976\n",
      " 6.794 6.03 ]\n",
      "type is: <class 'numpy.ndarray'>\n",
      "shape is: (506,)\n"
     ]
    }
   ],
   "source": [
    "# TODO - Assign x the values in the RM column - 5 points\n",
    "rms = df[\"RM\"]\n",
    "# print(rms)\n",
    "x = np.array(rms)\n",
    "x.reshape(1,506)\n",
    "print(x)\n",
    "print(\"type is:\", type(x))\n",
    "print(\"shape is:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scatter plot of the price vs. the `RM` attribute.  Make sure your plot has grid lines and label the axes with reasonable labels so that someone else can understand the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO - Create the scatter plot - 15 points \n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.grid()\n",
    "plt.xlabel(\"RM\")\n",
    "plt.ylabel(\"PRICES\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Simple Linear Model\n",
    "\n",
    "We will write a simple function to perform a linear fit. Use the formulae given in the class, to compute the parameters $\\beta_0,\\beta_1$ in the linear model $$y =\\beta_0 + \\beta_1 x + \\epsilon$$ as well as the coefficient of determination $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear(x,y):\n",
    "    \"\"\"\n",
    "    Given vectors of data points (x,y), performs a fit for the linear model:\n",
    "       yhat = w0 + w1*x, \n",
    "    The function returns w0, w1 and rsq, where rsq is the coefficient of determination.\n",
    "    \"\"\"\n",
    "    # TODO complete the following code\n",
    "    \n",
    "    # TODO - Find x_mean and y_mean - 5 points\n",
    "    x_mean = x.mean()\n",
    "    y_mean = y.mean()\n",
    "    \n",
    "    # TODO - Find sxx, sxy and syy - 5 points\n",
    "    sxx = np.var(x)\n",
    "    syy = np.var(y)\n",
    "    sxy = np.cov(x,y)[0][1]\n",
    "    \n",
    "    # TODO - Find w0 and w1 - 5 points\n",
    "    w1 = sxy/sxx\n",
    "    w0 = y_mean - w1 * x_mean\n",
    "    \n",
    "    # TODO - Find yhat - 5 points\n",
    "    yhat = w0 + w1*x\n",
    "    \n",
    "    # TODO - Find rss and tss - 5 points\n",
    "    rss = np.sum(np.square(y-yhat))\n",
    "    tss = np.sum(np.square(y_mean - y))\n",
    "    \n",
    "    # TODO - Find rsq - 5 points\n",
    "    rsq = 1 - (rss/tss)\n",
    "\n",
    "    return w0, w1, rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function `fit_linear` above, print the values `w0`, `w1` and `rsq` for the linear model of price vs. number of rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 = -34.78389488950894 | w1 = 9.120132959360863 | rsq = 0.483523559998957\n"
     ]
    }
   ],
   "source": [
    "# TODO - 5 points\n",
    "w0, w1, rsq = fit_linear(x, y)\n",
    "print(\"w0 =\",w0,\"| w1 =\", w1, \"| rsq =\", rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replot the scatter plot above, but now with the regression line.  You can create the regression line by creating points `xp` from say 4 to 9, computing the linear predicted values `yp` on those points and plotting `yp` vs. `xp` on top of the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.69663695 10.81676991 19.93690287 29.05703583 38.17716879 47.29730174]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEjhJREFUeJzt3X9sXXd5x/H3gxNUtxm4tGC1bli6UVlURcPUdGVBndPCXEYFHuq0so51U7cwCUq3MbNm2iZtYlsg22B/sB9Ry4g0ILAsdVHHalDTCwKVsqQG3JJ5YYWy3hZKRd3VxRpJ+uyPe9MlpYkdO+ce+37fL8nyPV+f4/M8uko+/p7vvedGZiJJKtfz6i5AklQvg0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUuDV1F7AYZ599dm7YsGHJxz/11FOcccYZp66gFa60fsGeS1Bav7D8nvft2/dYZr54of1WRRBs2LCBvXv3Lvn4RqPByMjIqStohSutX7DnEpTWLyy/54h4cDH7eWlIkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkrTATU002bt3DdPMJNm7dw8RUs9LzrYqXj0pSKSammmzZPc38wcOwHpqz82zZPQ3A2NBAJed0RiBJK8i2yZlWCBxl/uBhtk3OVHZOg0CSVpCHZ+dPavxUMAgkaQU5t6/3pMZPBYNAklaQ8dFBetf2HDPWu7aH8dHBys7pYrEkrSBHFoRbawJPMtDXy/joYGULxWAQSNKKMzY0wNjQAI1GgxuuHan8fF4akqTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwlQdBRPRExFRE3N7ePj8i7omIAxHxiYh4ftU1SJKOrxMzghuB/Udtvw/4QGZeADwOXN+BGiRJx1FpEETEecAbgZvb2wFcDuxq77IDGKuyBknSiVU9I/gg8B7g6fb2WcBsZh5qbz8EDFRcgyTpBNZU9Ysj4irg0czcFxEjR4afY9c8zvGbgc0A/f39NBqNJdcyNze3rONXm9L6BXsuQWn9Qgd7zsxKvoC/oPUX/7eA7wA/AD4KPAasae/zGmByod918cUX53Lcddddyzp+tSmt30x7LkFp/WYuv2dgby7i/+vKLg1l5pbMPC8zNwDXAHsy81rgLuDq9m7XAbdVVYMkaWF1vI/g94HfjYhv0FozuKWGGiRJbZWtERwtMxtAo/34AeCSTpxXkrQw31ksaUWbmGqycesepptPsHHrHiammnWX1HU6MiOQpKWYmGqyZfc08wcPw3pozs6zZfc0AGNDvvL8VHFGIGnF2jY50wqBo8wfPMy2yZmaKupOBoGkFevh2fmTGtfSGASSVqxz+3pPalxLYxBIWrHGRwfpXdtzzFjv2h7GRwdrqqg7uVgsacU6siDcWhN4koG+XsZHB10oPsUMAkkr2tjQAGNDAzQaDW64dqTucrqSl4YkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMJVFgQRcVpEfDkivhoR90fEn7THz4+IeyLiQER8IiKeX1UNkqSFVTkj+F/g8sz8KeCVwJURcSnwPuADmXkB8DhwfYU1SJIWUFkQZMtce3Nt+yuBy4Fd7fEdwFhVNUiSFhaZWd0vj+gB9gEvAz4EbAO+lJkva/98PfBvmXnRcxy7GdgM0N/ff/HOnTuXXMfc3Bzr1q1b8vGrTWn9gj2XoLR+Yfk9b9q0aV9mDi+035oln2ERMvMw8MqI6ANuBV7+XLsd59jtwHaA4eHhHBkZWXIdjUaD5Ry/2pTWL5TV88RUk22TM1yz/jA773ua8dFBxoYG6i6rciU9x0d0queOvGooM2eBBnAp0BcRRwLoPODhTtQgdYOJqSZbdk/TnJ0HoDk7z5bd00xMNWuuTKtZla8aenF7JkBE9AKvA/YDdwFXt3e7DritqhqkbrNtcob5g4ePGZs/eJhtkzM1VaRuUOWloXOAHe11gucBn8zM2yPi68DOiHgvMAXcUmENUld5uD0TWOy4tBiVBUFmfg0Yeo7xB4BLqjqv1M3O7et95rLQs8elpfKdxdIqMj46SO/anmPGetf2MD46WFNF6gaVvmpI0ql15NVBrTWBJxno6y3mVUOqjkEgrTJjQwOMDQ3QaDS44dqRustRF/DSkCQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVLhF3WsoIt4CvJbWx0p+ITNvrbQqSVLHLDgjiIi/BX4LmAbuA94eER+qujBJUmcsZkbws8BFmZkAEbGDVihIkrrAYtYIZoCXHrW9HvhaNeVIkjptMTOCs4D9EfHl9vargbsj4lMAmfmmqoqTJFVvMUGwC/gg8P2Ka5Ek1WAxQdAP3AjcC3wYmDyyXiBJWv0WXCPIzD8ELgBuAX4NOBARfx4RP1lxbZKkDljUG8raM4DvtL8OAWcCuyLi/RXWJknqgAUvDUXEu4DrgMeAm4HxzDwYEc8DDgDvqbZESVKVFrNGcDbwlsx88OjBzHw6Iq6qpixJUqcsGASZ+ccn+Nn+U1uOJKnTvOmcJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXCVBUFErI+IuyJif0TcHxE3tsdfFBGfjYgD7e9nVlWDJGlhVc4IDgHvzsyXA5cC74iIC4GbgDsz8wLgzva2tCQTU002bt3DdPMJNm7dw8RUs+6SpFWnsiDIzEcy89724yeB/cAA8GZgR3u3HcBYVTWou01MNdmye5rm7DwAzdl5tuyeNgykk9SRNYKI2AAMAfcA/Zn5CLTCAnhJJ2pQ99k2OcP8wcPHjM0fPMy2yZmaKpJWp6j6w8YiYh3wOeDPMnN3RMxmZt9RP388M39knSAiNgObAfr7+y/euXPnkmuYm5tj3bp1Sz5+tSml3+nmE8887u+F787//89eMfDCGirqrFKe5yNK6xeW3/OmTZv2ZebwQvtVGgQRsRa4ndbHW/51e2wGGMnMRyLiHKCRmYMn+j3Dw8O5d+/eJdfRaDQYGRlZ8vGrTSn9bty655nLQu9+xSH+arp1M92Bvl6+eNPldZbWEaU8z0eU1i8sv+eIWFQQVPmqoaD18Zb7j4RA26dofdAN7e+3VVWDutv46CC9a3uOGetd28P46An/rpD0LIv5YJql2gi8DZiOiK+0x/4A2Ap8MiKuB74N/GKFNaiLjQ0NALTXBJ5koK+X8dHBZ8YlLU5lQZCZXwDiOD++oqrzqixjQwOMDQ3QaDS44dqRusuRViXfWSxJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuEMgi4yMdVk49Y9TDefYOPWPUxMNesuSdIqsKbuAnRqTEw12bJ7mvmDh2E9NGfn2bJ7GoCxoYGaq5O0kjkj6BLbJmdaIXCU+YOH2TY5U1NFklaLyoIgIj4cEY9GxH1Hjb0oIj4bEQfa38+s6vyleXh2/qTGJemIKmcEHwGufNbYTcCdmXkBcGd7W6fAuX29JzUuSUdUFgSZ+Xng+88afjOwo/14BzBW1flLMz46SO/anmPGetf2MD46WFNFklaLTi8W92fmIwCZ+UhEvKTD5+9aRxaEW2sCTzLQ18v46KALxZIWFJlZ3S+P2ADcnpkXtbdnM7PvqJ8/npnPuU4QEZuBzQD9/f0X79y5c8l1zM3NsW7duiUfv9qU1i/YcwlK6xeW3/OmTZv2ZebwQvt1ekbw3Yg4pz0bOAd49Hg7ZuZ2YDvA8PBwjoyMLPmkjUaD5Ry/2pTWL9hzCUrrFzrXc6dfPvop4Lr24+uA2zp8fknSs1T58tGPA3cDgxHxUERcD2wFXh8RB4DXt7clSTWq7NJQZr71OD+6oqpzSpJOnu8slqTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkEklQ4g0CSCmcQSFLhDAJJKpxBIEmFMwgkqXAGgSQVziCQpMIZBJJUOINAkgpnEEhS4QwCSSqcQSBJhTMIJKlwBoEkFc4gkKTCGQSSVDiDQJIKZxBIUuG6Oggmppps3LqH6eYTbNy6h4mpZt0lSdKKs6buAqoyMdVky+5p5g8ehvXQnJ1ny+5pAMaGBmquTpJWjq6dEWybnGmFwFHmDx5m2+RMTRVJ0srUtUHw8Oz8SY1LUqm6NgjO7es9qXFJKlXXBsH46CC9a3uOGetd28P46GBNFUnSytS1i8VHFoRbawJPMtDXy/jooAvFkvQstcwIIuLKiJiJiG9ExE1VnWdsaIAv3nQ5rxh4IV+86XJDQJKeQ8eDICJ6gA8BbwAuBN4aERd2ug5JUksdM4JLgG9k5gOZ+UNgJ/DmGuqQJAGRmZ09YcTVwJWZ+Rvt7bcBP52Z73zWfpuBzQD9/f0X79y5c8nnnJubY926dUsvepUprV+w5xKU1i8sv+dNmzbty8zhhfarY7E4nmPsR9IoM7cD2wGGh4dzZGRkySdsNBos5/jVprR+wZ5LUFq/0Lme67g09BCw/qjt84CHa6hDkkQ9l4bWAP8JXAE0gX8Hfjkz7z/BMd8DHlzGac8GHlvG8atNaf2CPZegtH5h+T3/eGa+eKGdOn5pKDMPRcQ7gUmgB/jwiUKgfcyCjZxIROxdzHWyblFav2DPJSitX+hcz7W8oSwzPw18uo5zS5KO1bW3mJAkLU4pQbC97gI6rLR+wZ5LUFq/0KGeO75YLElaWUqZEUiSjqPrgyAieiJiKiJur7uWToiIb0XEdER8JSL21l1P1SKiLyJ2RcR/RMT+iHhN3TVVKSIG28/tka//iYjfrruuqkXE70TE/RFxX0R8PCJOq7umKkXEje1e7+/E89u1t6E+yo3AfuAFdRfSQZsys5TXW/8NcEdmXh0RzwdOr7ugKmXmDPBKeOYGjk3g1lqLqlhEDADvAi7MzPmI+CRwDfCRWgurSERcBPwmrfuy/RC4IyL+NTMPVHXOrp4RRMR5wBuBm+uuRadeRLwAuAy4BSAzf5iZs/VW1VFXAP+Vmct5s+VqsQbobb8h9XS6+24ELwe+lJk/yMxDwOeAX6jyhF0dBMAHgfcAT9ddSAcl8JmI2Ne+cV83+wnge8A/ti//3RwRZ9RdVAddA3y87iKqlplN4C+BbwOPAE9k5mfqrapS9wGXRcRZEXE68PMce1ueU65rgyAirgIezcx9ddfSYRsz81W0Pu/hHRFxWd0FVWgN8Crg7zJzCHgKqOyDjlaS9mWwNwH/XHctVYuIM2ndqv584FzgjIj4lXqrqk5m7gfeB3wWuAP4KnCoynN2bRAAG4E3RcS3aH3mweUR8U/1llS9zHy4/f1RWteOL6m3oko9BDyUmfe0t3fRCoYSvAG4NzO/W3chHfA64JuZ+b3MPAjsBn6m5poqlZm3ZOarMvMy4PtAZesD0MVBkJlbMvO8zNxAawq9JzO79q8IgIg4IyJ+7Mhj4OdoTTO7UmZ+B/jviBhsD10BfL3GkjrprRRwWajt28ClEXF6RASt53l/zTVVKiJe0v7+UuAtVPxcl/CqoZL0A7e2/q2wBvhYZt5Rb0mVuwH4aPtSyQPAr9dcT+Xa141fD7y97lo6ITPviYhdwL20LpFM0f3vMv6XiDgLOAi8IzMfr/JkvrNYkgrXtZeGJEmLYxBIUuEMAkkqnEEgSYUzCCSpcAaBJBXOIJCkwhkE0iJExKsj4msRcVr7Hdz3R8Q7I+LzEXFrRHw9Iv4+Ivw3pVXHN5RJixQR7wVOA3pp3efoblo3BbsQeLD9+B8yc1dtRUpL4F8v0uL9Ka1bOwwD72+PfTkzH8jMw7TuB/PauoqTlsp7DUmL9yJgHbCW1swAWp//cDSn2Fp1nBFIi7cd+CPgo7TuFw9wSUSc314b+CXgC3UVJy2VQSAtQkT8KnAoMz8GbAVeTevfz93t7fuAb9Llnx+s7uRisbREETEC/F5mXlV3LdJyOCOQpMI5I5CkwjkjkKTCGQSSVDiDQJIKZxBIUuEMAkkqnEEgSYX7P3mays2WS9hvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO - Replot scatter plot with regression line - 15 points\n",
    "xp = np.arange(4,10)\n",
    "yp = w0 + w1 * xp\n",
    "print(yp)\n",
    "plt.scatter(xp, yp)\n",
    "plt.grid()\n",
    "plt.xlabel(\"xp\")\n",
    "plt.ylabel(\"yp\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute coefficients of determination\n",
    "\n",
    "We next compute the $R^2$ values for all the predictors and output the values in a table. Your table should look like the following, where each the first column is the attribute name and the second column is the $R^2$ value.\n",
    "\n",
    "    CRIM        0.151\n",
    "    ZN          0.130\n",
    "    INDUS       0.234\n",
    "    ...         ...\n",
    "\n",
    "To index over the set of colunms in the dataframe `df`, you can either loop over the items in the `names` lists (skipping over the final name `PRICE`) or loop over integer indices and use the method, `df.iloc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM         0.1507798778117646\n",
      "ZN         0.12992033545072557\n",
      "INDUS         0.2339891129264654\n",
      "CHAS         0.030716009265172373\n",
      "NOX         0.1826023264815343\n",
      "RM         0.483523559998957\n",
      "AGE         0.14209418689813957\n",
      "DIS         0.06246412718747363\n",
      "RAD         0.14563800883662703\n",
      "TAX         0.2195250602426465\n",
      "PTRATIO         0.2578463069423942\n",
      "B         0.11119568222590093\n",
      "LSTAT         0.5441441638886164\n"
     ]
    }
   ],
   "source": [
    "# TODO - Find rsq values for all columns - 10 points \n",
    "for i in range(13):\n",
    "    x_axis = df.iloc[:, i]\n",
    "    value = fit_linear(x_axis, y)\n",
    "    print(df.columns[i], \"       \", value[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "You have finished the first part of the asssignment, feel free to take a break and come back to do the second part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Linear Regression Using Gradient Descent \n",
    "\n",
    "In this lab, you will be implementing the Linear Regression Model. We will be using the gradient descent algorithm (GDA) and the stochastic gradient descent algorithm (SGDA) to minimize the cost function as we covered in the class.\n",
    "\n",
    "Please add your own print statements to check your code to ensure your code is correct in every step. (Note: we will not be grading the print statements you add to your code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the values from the 'PRICE' and 'RM' columns. into a smaller dataframe named df1 from df. This step is the same as was done in programming_assignment_1a.  Then drop rows with NaN values from df1, and save the result in df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PRICE     RM\n",
      "0     24.0  6.575\n",
      "1     21.6  6.421\n",
      "2     34.7  7.185\n",
      "3     33.4  6.998\n",
      "4     36.2  7.147\n",
      "5     28.7  6.430\n",
      "6     22.9  6.012\n",
      "7     27.1  6.172\n",
      "8     16.5  5.631\n",
      "9     18.9  6.004\n",
      "10    15.0  6.377\n",
      "11    18.9  6.009\n",
      "12    21.7  5.889\n",
      "13    20.4  5.949\n",
      "14    18.2  6.096\n",
      "15    19.9  5.834\n",
      "16    23.1  5.935\n",
      "17    17.5  5.990\n",
      "18    20.2  5.456\n",
      "19    18.2  5.727\n",
      "20    13.6  5.570\n",
      "21    19.6  5.965\n",
      "22    15.2  6.142\n",
      "23    14.5  5.813\n",
      "24    15.6  5.924\n",
      "25    13.9  5.599\n",
      "26    16.6  5.813\n",
      "27    14.8  6.047\n",
      "28    18.4  6.495\n",
      "29    21.0  6.674\n",
      "..     ...    ...\n",
      "476   16.7  6.484\n",
      "477   12.0  5.304\n",
      "478   14.6  6.185\n",
      "479   21.4  6.229\n",
      "480   23.0  6.242\n",
      "481   23.7  6.750\n",
      "482   25.0  7.061\n",
      "483   21.8  5.762\n",
      "484   20.6  5.871\n",
      "485   21.2  6.312\n",
      "486   19.1  6.114\n",
      "487   20.6  5.905\n",
      "488   15.2  5.454\n",
      "489    7.0  5.414\n",
      "490    8.1  5.093\n",
      "491   13.6  5.983\n",
      "492   20.1  5.983\n",
      "493   21.8  5.707\n",
      "494   24.5  5.926\n",
      "495   23.1  5.670\n",
      "496   19.7  5.390\n",
      "497   18.3  5.794\n",
      "498   21.2  6.019\n",
      "499   17.5  5.569\n",
      "500   16.8  6.027\n",
      "501   22.4  6.593\n",
      "502   20.6  6.120\n",
      "503   23.9  6.976\n",
      "504   22.0  6.794\n",
      "505   11.9  6.030\n",
      "\n",
      "[506 rows x 2 columns]\n",
      "(506, 2)\n"
     ]
    }
   ],
   "source": [
    "#  After completing the code in this code cell, run this code cell before moving further. \n",
    "\n",
    "# TODO - Save RM and PRICE values into df1 from df - 10 points \n",
    "frames = df[[\"PRICE\", \"RM\"]] #index as a dataframe\n",
    "\n",
    "df1 = pd.DataFrame(frames)\n",
    "\n",
    "print(df1)\n",
    "\n",
    "# TODO - Remove all the nan values from df1 and save into df2 - 10 points.\n",
    "df2=df1.dropna()\n",
    "\n",
    "# Check the shape of df2. It should be (506,2)\n",
    "print(df2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector y having the values of 'PRICE' column and vector x having the values of 'RM' column. This step is also same as done in programming_assignment_1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# TODO - Save 'PRICE' column in y and 'RM' in x - 5 points \n",
    "\n",
    "y = np.array(df[\"PRICE\"])\n",
    "x = np.array(df[\"RM\"]) \n",
    "\n",
    "\n",
    "\n",
    "# Check the shape of x and y vectors - should be (506, )\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape x and y to be rank 2 (here we refer to the number of dimentions of a array as it's rank). After checking the shape of x and y in the above code cell, we see that x and y are rank 1 matricies. Before proceeding,  convert them to be rank 2 matricies. For example, you could use the command x=x.reshape(x.shape[0],1) to reshape x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# TODO - Reshape x and y into rank 2 matrices - 5 points\n",
    "x=x.reshape(x.shape[0], 1)\n",
    "print(x.shape)\n",
    "y = y.reshape(y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Write the code to perform gradient descent in the function below.  In this assigment, we will run the algorithm a fixed number of iterations.  Later in the course, we will add a simple extension by adding a stopping criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the value of N, i.e. number of training examples. \n",
    "Hint: Value of n is equal to the number of rows in either x or y matrix which can be accessed using the numpy shape command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506\n"
     ]
    }
   ],
   "source": [
    "# TODO - Save number of training examples in N and print it - 5 points \n",
    "N = x.shape[0]\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost: Write the code to compute the cost inside the function. Do not change the function name or function parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w0, w1, N):\n",
    "    # Write your code in place of None. Cost can be calculated using a single line of code\n",
    "    # TODO - Write the formula for the cost - 10 points\n",
    "    \n",
    "    cost = np.sum(np.square(w0 + w1*x - y))/(2*N)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, ensure that the code you have written to compute the cost is correct. Just run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296.0734584980237\n"
     ]
    }
   ],
   "source": [
    "cost_verify= compute_cost(x, y, 0, 0, N)\n",
    "print(cost_verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be equal to 296.073458498.  Ensure your answer is correct before continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, learning_rate, w0, w1, N, num_iters):\n",
    "    # In place of None, write the updated value of w0 in temp0 and of w1 in temp1\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        # TODO - Write the update rules for w0 and w1 using gradient descent - 20 points\n",
    "        \n",
    "        temp0 = w0 - ((learning_rate/N)*np.sum(w0 + x*w1 - y ))\n",
    "        temp1 = w1 - (learning_rate/N)*np.sum((w0 + x*w1 - y )*x) \n",
    "        w0 = temp0\n",
    "        w1 = temp1\n",
    "        \n",
    "        if(i%100==0):\n",
    "            # In place of None, call the cost you just coded above\n",
    "            cost= compute_cost(x, y, w0, w1, N)\n",
    "#             print(\"Cost\")\n",
    "#             print(cost)\n",
    "#             print(\"w's\")\n",
    "#             print(w0)\n",
    "#             print(w1) \n",
    "            \n",
    "    return w0,w1       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, ensure that your code to update w0 and w1 is correct. Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.3837376153493\n"
     ]
    }
   ],
   "source": [
    "g=gradient_descent(x, y, 0.04, 0, 0, N, 10000)\n",
    "print(g[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last output should be: -34.3837376153. Ensure that you have the correct result before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integerating the Batch Gradient Descent Function \n",
    "\n",
    "Using the above code, create a single function linear_reg_model_gda: This function uses the gradient descent algorithm to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg_model_gda(x, y, N, learning_rate, num_iters):\n",
    "    #initialize the values of parameters w0 and w1 both to 0\n",
    "    \n",
    "    # TODO - Set w0 and w1 as 0 - 5 points\n",
    "    w0= 0\n",
    "    w1= 0\n",
    "    \n",
    "    # TODO - Print initial cost - 5 points\n",
    "    initial_cost= compute_cost(x, y, w0, w1, N)\n",
    "    print(\"Initial Cost\")\n",
    "    print(initial_cost)\n",
    "    \n",
    "    # TODO - Calculate the optimized value of w0 and w1 by calling the gradient_descent function coded above - 5 points\n",
    "    w0,w1= gradient_descent(x, y, learning_rate, w0, w1, N, num_iters)\n",
    "    \n",
    "    # TODO - Calculate the cost with the optimized value of w0 and w1 by calling the cost function - 5 points\n",
    "    final_cost= compute_cost(x, y, w0, w1, N)\n",
    "    print(\"Final Cost\")\n",
    "    print(final_cost)\n",
    "    return w0,w1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you have completed the linear_reg_model_gda function, you can use this function to find the optimized values of w0 and w1. Using it, set the values of learning_rate and num_iters to something reasonable. You may have to call this function several times with different values of num_iters and learning_rate to find the optimal values of w0 and w1. For some values of learning_rate, you may recieve incorrect values of w0 and w1, wherein they reach a very large value(infinity) due to overshooting as was discussed in class. Finally, the values of w0, w1 and cost(RSS) should be same(or very close to) the ones in Part 1 of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost\n",
      "296.0734584980237\n",
      "Final Cost\n",
      "21.80027588558478\n",
      "-34.67062077642869 9.102108981178757\n"
     ]
    }
   ],
   "source": [
    "# Write your code below - change the values of num_iters and learning_rate.\n",
    "learning_rate=0.03\n",
    "num_iters=100000\n",
    "\n",
    "# TODO - Call the linear_reg_model_gda function - 5 points\n",
    "w0,w1 = linear_reg_model_gda(x, y, N, learning_rate, num_iters)\n",
    "print(w0,w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Price of a House"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your function to train your linear regression model to find the optimal values for $w_0$ and $w_1$.  Once you have the optimal values for the parameters, you can predict the value of $y$ (price) using $x$ (rm).  Compute the function below to prdict $y$, using $x$, $w_0$ and $w_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w0, w1):\n",
    "    predicted_y = w0 + w1 * x\n",
    "    \n",
    "    return predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, predict the price of a house with rm=6 using ```predict()```. The value should be ~19.54424 ± 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.942033110643848\n"
     ]
    }
   ],
   "source": [
    "# TODO - Call the predict function with rm=6 - 5 points\n",
    "y_predict = predict(6, w0, w1)\n",
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation Method\n",
    "Now, we will be writing the code to find the values of parameters w0 and w1 for our linear regression model. This can also be used to cross-check the optimal values of w0 and w1 we just found above using the above two models. These values should be approximately the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Appending a column of ones to x in the left. Save this into X. You can first create a column vector of ones say 'a' (ensure this to have dimension (N,1) i.e. a rank 2 array). Now, you can use np.hstack to append it to the left of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create a and X matrices - 5 points \n",
    "a = np.ones((N,1))\n",
    "X = np.hstack((a, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Instead of writing the code for normal equation in one line, you can break this into 3 parts: First calculate q=inverse of (dot of (X.T,X)) (these are pseudo commands, use original numpy commands to calculate q). Then w= dot of ( X.T , y) and then w_vec= dot of (q,w). Here, w_vec is vector of dimension (2,1) having two values. Example w0=w_vec[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-34.67062078]\n",
      " [  9.10210898]]\n",
      "w0 is [-34.67062078]\n",
      "w1 is [9.10210898]\n"
     ]
    }
   ],
   "source": [
    "# TODO - Write the normal equation formulas - 10 points\n",
    "\n",
    "q = inv(np.dot(np.transpose(X),X))\n",
    "w = np.dot(np.transpose(X), y)\n",
    "w_vec = np.dot(q,w)\n",
    "print(w_vec)\n",
    "w0 = w_vec[0]\n",
    "w1 = w_vec[1]\n",
    "print(\"w0 is \" + str(w0))\n",
    "print(\"w1 is \" + str(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Values of w0 and w1 you just got above should be approximately the same as the ones you got using linear_reg_model_gda or linear_reg_model_sgda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Optional (The following will not be graded):  \n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "You can read more about stochastic gradient descent: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "\n",
    "To prevent cycles, shuffle the data for each pass.\n",
    "\n",
    "Write the code to perform a stochastic gradient descent. Remember, every update in a sgda uses examples one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x, y, learning_rate, w0, w1, N, num_iters):\n",
    "\n",
    "    for j in range(num_iters):\n",
    "    \n",
    "        for i in range(0,N):\n",
    "        # Write updated value in w0 in temp0 and of w1 in temp1\n",
    "            temp0 = w0 - ((learning_rate/N)*np.sum(w0 + x*w1 - y ))\n",
    "            temp1 = w1 - (learning_rate/N)*np.sum((w0 + x*w1 - y )*x) \n",
    "            w0 =temp0\n",
    "            w1 = temp1   \n",
    "\n",
    "        if(j%2000==0):\n",
    "            cost= compute_cost(x, y, w0, w1, N)\n",
    "#             print(\"Cost\")\n",
    "#             print(cost)\n",
    "#             print(\"W\")\n",
    "#             print(w0)\n",
    "#             print(w1)           \n",
    "            \n",
    "    return w0,w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, ensure that your code to update w0 and w1 is correct. Run the code cell below. This may take some time to run completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g=stochastic_gradient_descent(x, y, 0.004, 0, 0, N, 100)\n",
    "print(\"g[0]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be: -17.98968896 . Ensure that your answer is correct before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating the Stochastic Gradient Descent Algorithm\n",
    "\n",
    "Use this function to complete linear_reg_model_sgda(). This function uses stochastic gradient descent to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg_model_sgda(x, y, N, learning_rate, num_iters):\n",
    "    \n",
    "    w0=0\n",
    "    w1=0\n",
    "    \n",
    "    #calculate the initial cost by calling the function cost you just coded above\n",
    "    print(\"Initial Cost\")\n",
    "    initial_cost=None\n",
    "    print(initial_cost)\n",
    "    \n",
    "    #calculate the optimized value of w0 and w1 by calling the stochastic_gradient_descent function coded above\n",
    "    \n",
    "    w0,w1= None\n",
    "    \n",
    "    #Calculate the cost with the optimized value of w0 and w1 by calling the cost function.\n",
    "    \n",
    "    final_cost=None\n",
    "    print(\"Final_cost\")\n",
    "    print(final_cost)\n",
    "    return w0,w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you have completed linear_reg_model_sgda function, you can call this function to find the optimized values of w0 and w1. Before calling the function, set the values of learning_rate and num_iters appropriately. You may have to call this function several times with different values of num_iters and learning_rate to find the optimal values of w0 and w1. For a sufficiently high learning_rate, it may return extremely high values for w0 and w1 (infinity). Finally, the values of w0, w1 and cost(RSS) should be same(or nearly the same) as you got in the programming_assignment1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "learning_rate = 0.005\n",
    "num_iters = 50000\n",
    "# In place of None call the function linear_reg_model_sgda.\n",
    "w0,w1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
